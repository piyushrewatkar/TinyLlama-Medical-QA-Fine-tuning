{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TinyLlama Medical QA Fine-tuning on Apple Silicon (Fixed)\n",
    "\n",
    "This notebook demonstrates how to fine-tune TinyLlama-1.1B-Chat-v1.0 on medical QA datasets using LoRA (Low-Rank Adaptation) on Apple Silicon Macs (M1/M2/M3). The implementation is optimized for MPS (Metal Performance Shaders) backend when available.\n",
    "\n",
    "## Key Features:\n",
    "- 4-bit quantization for memory efficiency\n",
    "- LoRA for parameter-efficient fine-tuning\n",
    "- Apple Silicon MPS optimization\n",
    "- Medical QA dataset from lavita/medical-qa-datasets\n",
    "- Comprehensive error handling and device detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Requirements and Installation\n",
    "\n",
    "**Important**: This notebook requires a properly configured Python environment with all necessary packages. Please run the provided setup script before using this notebook:\n",
    "\n",
    "```bash\n",
    "# Run this in your terminal\n",
    "bash setup_macos_env.sh\n",
    "```\n",
    "\n",
    "After running the script:\n",
    "1. Activate the virtual environment: `source tinyllama_env/bin/activate`\n",
    "2. Start Jupyter: `jupyter notebook`\n",
    "3. Open this notebook and select the kernel: **Kernel > Change kernel > TinyLlama Fine-tuning**\n",
    "\n",
    "The setup script installs:\n",
    "- PyTorch with MPS support\n",
    "- Transformers 4.36.2\n",
    "- PEFT 0.7.1\n",
    "- TRL 0.7.10\n",
    "- Other required dependencies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Environment Setup and Device Detection\n",
    "\n",
    "Detect and configure the appropriate device (MPS for Apple Silicon or CPU fallback):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Using Apple Silicon GPU (MPS) on i386\n",
      "MPS available memory: 0.00 GB\n",
      "Memory growth enabled for MPS\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import platform\n",
    "import warnings\n",
    "import os\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set environment variable for MPS memory management\n",
    "os.environ[\"PYTORCH_MPS_HIGH_WATERMARK_RATIO\"] = \"0.0\"\n",
    "\n",
    "# Device detection for Apple Silicon\n",
    "def get_device():\n",
    "    \"\"\"Detect and return the best available device for Apple Silicon Macs\"\"\"\n",
    "    if torch.backends.mps.is_available():\n",
    "        # Check if MPS is built\n",
    "        if torch.backends.mps.is_built():\n",
    "            device = torch.device(\"mps\")\n",
    "            print(f\"✅ Using Apple Silicon GPU (MPS) on {platform.processor()}\")\n",
    "            print(f\"MPS available memory: {torch.mps.driver_allocated_memory() / 1024**3:.2f} GB\")\n",
    "            return device\n",
    "    \n",
    "    # Fallback to CPU\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(f\"⚠️  MPS not available, using CPU on {platform.processor()}\")\n",
    "    return device\n",
    "\n",
    "# Set device\n",
    "device = get_device()\n",
    "\n",
    "# Set memory growth for MPS if available\n",
    "if device.type == \"mps\":\n",
    "    # Enable memory growth to prevent OOM errors\n",
    "    torch.mps.set_per_process_memory_fraction(1.0)\n",
    "    print(\"Memory growth enabled for MPS\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Import Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig,\n",
    "    TrainingArguments,\n",
    "    DataCollatorForLanguageModeling,\n",
    "    Trainer,\n",
    "    pipeline\n",
    ")\n",
    "from peft import LoraConfig, PeftModel, prepare_model_for_kbit_training, get_peft_model\n",
    "from datasets import load_dataset\n",
    "from trl import SFTTrainer\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from typing import Dict, List"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Model and Tokenizer Loading\n",
    "\n",
    "Load TinyLlama with appropriate configuration for Apple Silicon:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quantization disabled for MPS - using float32 for stability\n",
      "Added special tokens: ['<|system|>', '<|user|>', '<|assistant|>']\n",
      "Model loaded successfully on mps\n",
      "Model size: 1.10B parameters\n"
     ]
    }
   ],
   "source": [
    "# Model configuration\n",
    "model_name = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n",
    "\n",
    "# Note: BitsAndBytes quantization is not fully supported on MPS yet\n",
    "# We'll use a different approach for Apple Silicon\n",
    "use_quantization = device.type != \"mps\"  # Only use quantization on non-MPS devices\n",
    "\n",
    "if use_quantization:\n",
    "    # 4-bit quantization config for non-Apple Silicon\n",
    "    bnb_config = BitsAndBytesConfig(\n",
    "        load_in_4bit=True,\n",
    "        bnb_4bit_quant_type=\"nf4\",\n",
    "        bnb_4bit_compute_dtype=torch.float16,\n",
    "        bnb_4bit_use_double_quant=True\n",
    "    )\n",
    "    print(\"Using 4-bit quantization\")\n",
    "else:\n",
    "    bnb_config = None\n",
    "    print(\"Quantization disabled for MPS - using float32 for stability\")\n",
    "\n",
    "# Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\"\n",
    "\n",
    "# Load model with appropriate configuration\n",
    "if device.type == \"mps\":\n",
    "    # For Apple Silicon, use float32 for training stability\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_name,\n",
    "        torch_dtype=torch.float32,  # Use float32 for MPS stability\n",
    "        device_map={\"\":0} if device.type == \"cuda\" else None,\n",
    "        trust_remote_code=True\n",
    "    )\n",
    "    model = model.to(device)\n",
    "else:\n",
    "    # For other devices, use quantization\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_name,\n",
    "        quantization_config=bnb_config,\n",
    "        device_map=\"auto\",\n",
    "        trust_remote_code=True\n",
    "    )\n",
    "\n",
    "# ───  Add chat special-tokens ────────────────────────────────────────────  \n",
    "chat_tokens = [\"<|system|>\", \"<|user|>\", \"<|assistant|>\"]\n",
    "\n",
    "# Check if special tokens already exist\n",
    "existing_special_tokens = tokenizer.special_tokens_map.get(\"additional_special_tokens\", [])\n",
    "tokens_to_add = [token for token in chat_tokens if token not in existing_special_tokens]\n",
    "\n",
    "if tokens_to_add:\n",
    "    tokenizer.add_special_tokens({\"additional_special_tokens\": existing_special_tokens + tokens_to_add})\n",
    "    model.resize_token_embeddings(len(tokenizer))\n",
    "    print(f\"Added special tokens: {tokens_to_add}\")\n",
    "\n",
    "# align pad token & disable cache for gradient checkpointing\n",
    "model.config.pad_token_id = tokenizer.pad_token_id\n",
    "model.config.use_cache = False\n",
    "\n",
    "print(f\"Model loaded successfully on {device}\")\n",
    "print(f\"Model size: {sum(p.numel() for p in model.parameters()) / 1e9:.2f}B parameters\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Dataset Loading and Preprocessing\n",
    "\n",
    "Load and preprocess the medical QA dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset size: 239357 examples\n",
      "Dataset features: {'instruction': Value(dtype='string', id=None), 'input': Value(dtype='string', id=None), 'output': Value(dtype='string', id=None), '__index_level_0__': Value(dtype='int64', id=None)}\n",
      "\n",
      "Sample data:\n",
      "instruction: If you are a doctor, please answer the medical questions based on the patient's description....\n",
      "input: hi. im a home health aide and i have a client with scoliosis in the back and kidney disease. her feet ankles and calves have been swollen for the past 2 weeks. mostly in her feet. she started a patch ...\n",
      "output: hi, thanks for contacting chatbot. swelling in the legs and feet can come from many causes, one of them being general circulation or ineffectiveness of the kidneys to rid the body of excess water. mos...\n",
      "__index_level_0__: 137213...\n",
      "Using reduced dataset size for MPS: 239357 examples\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "37ef5004219546b4bb5c96426fb8816d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/215421 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "edb94499c5fe48729482fca521960ad0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/23936 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "58a82c1bd0504ac49cfaefe4fe41eb1d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/215421 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b739429273ea484abbdada3c3dd45285",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/23936 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train rows: 214934\n",
      "eval  rows: 23885\n",
      "Training examples after tag/length filter: 214934\n",
      "Validation examples after tag/length filter: 23885\n"
     ]
    }
   ],
   "source": [
    "# Load medical QA dataset\n",
    "# Note: This dataset has multiple configurations, let's use the default one\n",
    "dataset = load_dataset(\"lavita/medical-qa-datasets\", 'all-processed', split=\"train\")\n",
    "print(f\"Dataset size: {len(dataset)} examples\")\n",
    "print(f\"Dataset features: {dataset.features}\")\n",
    "\n",
    "# Display sample to understand the structure\n",
    "print(\"\\nSample data:\")\n",
    "sample = dataset[0]\n",
    "for key, value in sample.items():\n",
    "    print(f\"{key}: {value[:200] if isinstance(value, str) else value}...\")\n",
    "\n",
    "# Preprocessing function for medical QA\n",
    "def preprocess_medical_qa(examples):\n",
    "    \"\"\"Format medical QA data for TinyLlama chat format\"\"\"\n",
    "    texts = []\n",
    "    \n",
    "    # Check which fields are available in the dataset\n",
    "    # Common variations: 'input'/'output', 'question'/'answer', 'text'/'response'\n",
    "    if 'input' in examples and 'output' in examples:\n",
    "        question_field = 'input'\n",
    "        answer_field = 'output'\n",
    "    elif 'question' in examples and 'answer' in examples:\n",
    "        question_field = 'question'\n",
    "        answer_field = 'answer'\n",
    "    elif 'text' in examples and 'response' in examples:\n",
    "        question_field = 'text'\n",
    "        answer_field = 'response'\n",
    "    elif 'instruction' in examples and 'response' in examples:\n",
    "        question_field = 'instruction'\n",
    "        answer_field = 'response'\n",
    "    else:\n",
    "        # If standard fields not found, print available fields\n",
    "        print(f\"Available fields: {list(examples.keys())}\")\n",
    "        # Try to use first two fields as question/answer\n",
    "        fields = list(examples.keys())\n",
    "        if len(fields) >= 2:\n",
    "            question_field = fields[0]\n",
    "            answer_field = fields[1]\n",
    "            print(f\"Using '{question_field}' as question and '{answer_field}' as answer\")\n",
    "        else:\n",
    "            # If dataset has only one field, it might be pre-formatted\n",
    "            if 'text' in examples:\n",
    "                return examples  # Return as-is if already formatted\n",
    "            raise ValueError(f\"Cannot determine question/answer fields from: {list(examples.keys())}\")\n",
    "    \n",
    "    for i in range(len(examples[question_field])):\n",
    "        # TinyLlama chat format\n",
    "        text = f\"\"\"<|system|>\n",
    "You are a helpful medical assistant. Answer the medical question accurately and concisely.</s>\n",
    "<|user|>\n",
    "{examples[question_field][i]}</s>\n",
    "<|assistant|>\n",
    "{examples[answer_field][i]}</s>\"\"\"\n",
    "        texts.append(text)\n",
    "    \n",
    "    return {\"text\": texts}\n",
    "\n",
    "# Apply preprocessing\n",
    "dataset = dataset.map(preprocess_medical_qa, batched=True)\n",
    "\n",
    "# Split dataset for training and validation\n",
    "# Use smaller subset for Apple Silicon due to memory constraints\n",
    "if device.type == \"mps\":\n",
    "    # Use 500 samples for MPS to avoid memory issues\n",
    "    dataset = dataset.select(range(len(dataset)))\n",
    "    print(f\"Using reduced dataset size for MPS: {len(dataset)} examples\")\n",
    "\n",
    "# Create train/validation split\n",
    "train_test_split = dataset.train_test_split(test_size=0.1, seed=42)\n",
    "train_dataset = train_test_split[\"train\"]\n",
    "eval_dataset = train_test_split[\"test\"]\n",
    "\n",
    "# ─── tokenise ➊ build labels ➋ keep only rows with ≥1 real label ─────────\n",
    "max_len = model.config.max_position_embeddings  # 2048\n",
    "assistant_id = tokenizer.convert_tokens_to_ids(\"<|assistant|>\")\n",
    "pad_token_id = tokenizer.pad_token_id  # = eos(</s>)\n",
    "\n",
    "def tokenize_and_label(example):\n",
    "    enc = tokenizer(\n",
    "        example[\"text\"],\n",
    "        truncation=True,\n",
    "        max_length=max_len,\n",
    "        padding=\"max_length\",\n",
    "    )\n",
    "    ids = enc[\"input_ids\"]\n",
    "    mask = enc[\"attention_mask\"]  # 1 = real token, 0 = pad\n",
    "\n",
    "    # find last \"<|assistant|>\" that is still a *real* token (mask == 1)\n",
    "    try:\n",
    "        tag_pos = max(i for i, tid in enumerate(ids) if tid == assistant_id and mask[i] == 1)\n",
    "    except ValueError:\n",
    "        enc[\"keep\"] = False  # tag missing\n",
    "        enc[\"labels\"] = [-100] * max_len\n",
    "        return enc\n",
    "\n",
    "    # any real token after the tag?\n",
    "    if tag_pos + 1 >= max_len or mask[tag_pos + 1] == 0:\n",
    "        enc[\"keep\"] = False  # answer truncated away\n",
    "        enc[\"labels\"] = [-100] * max_len\n",
    "        return enc\n",
    "\n",
    "    # build labels: copy *only* real tokens after the tag\n",
    "    labels = [-100] * max_len\n",
    "    for i in range(tag_pos + 1, max_len):\n",
    "        if mask[i] == 0:  # stop at first pad\n",
    "            break\n",
    "        labels[i] = ids[i]\n",
    "    enc[\"labels\"] = labels\n",
    "    enc[\"keep\"] = True\n",
    "    return enc\n",
    "\n",
    "# map → always returns input_ids/attention_mask/labels/keep\n",
    "train_dataset = train_dataset.map(tokenize_and_label, remove_columns=train_dataset.column_names)\n",
    "eval_dataset = eval_dataset.map(tokenize_and_label, remove_columns=eval_dataset.column_names)\n",
    "\n",
    "# filter rows where keep == True\n",
    "train_dataset = train_dataset.filter(lambda x: x[\"keep\"], batched=False).remove_columns(\"keep\")\n",
    "eval_dataset = eval_dataset.filter(lambda x: x[\"keep\"], batched=False).remove_columns(\"keep\")\n",
    "\n",
    "print(\"train rows:\", len(train_dataset))\n",
    "print(\"eval  rows:\", len(eval_dataset))\n",
    "\n",
    "print(f\"Training examples after tag/length filter: {len(train_dataset)}\")\n",
    "print(f\"Validation examples after tag/length filter: {len(eval_dataset)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. LoRA Configuration\n",
    "\n",
    "Configure LoRA for parameter-efficient fine-tuning optimized for Apple Silicon:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'NoneType' object has no attribute 'cadam32bit_grad_fp32'\n",
      "trainable params: 12,615,680 || all params: 1,112,676,352 || trainable%: 1.1338139772022404\n"
     ]
    }
   ],
   "source": [
    "# Prepare model for k-bit training if using quantization\n",
    "if use_quantization:\n",
    "    model = prepare_model_for_kbit_training(model)\n",
    "\n",
    "if device.type == \"cuda\":\n",
    "    model = torch.compile(model, backend=\"aot_eager\")\n",
    "\n",
    "# LoRA configuration optimized for Apple Silicon\n",
    "peft_config = LoraConfig(\n",
    "    r=16,\n",
    "    lora_alpha=32,\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    "    target_modules=[\n",
    "        \"q_proj\", \"k_proj\", \"v_proj\",\n",
    "        \"o_proj\",\n",
    "        \"gate_proj\", \"up_proj\", \"down_proj\",\n",
    "    ],\n",
    ")\n",
    "\n",
    "# Get PEFT model\n",
    "model = get_peft_model(model, peft_config)\n",
    "model.print_trainable_parameters()\n",
    "\n",
    "# Enable gradient checkpointing for memory efficiency\n",
    "model.enable_input_require_grads()\n",
    "if hasattr(model, \"gradient_checkpointing_enable\"):\n",
    "    model.gradient_checkpointing_enable()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Training Configuration\n",
    "\n",
    "Set up training arguments optimized for Apple Silicon:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Effective batch size: 4\n",
      "Total training steps: 500\n"
     ]
    }
   ],
   "source": [
    "# Training arguments optimized for Apple Silicon\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./tinyllama-medical-qa-lora\",\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=2 if device.type == \"mps\" else 4,  # Even smaller batch for MPS\n",
    "    per_device_eval_batch_size=2 if device.type == \"mps\" else 4,\n",
    "    gradient_accumulation_steps=2 if device.type == \"mps\" else 2,  # Reduced for stability\n",
    "    gradient_checkpointing=True,\n",
    "    gradient_checkpointing_kwargs={\"use_reentrant\": False} if device.type == \"mps\" else {},\n",
    "    optim=\"adamw_torch\" if device.type == \"mps\" else \"paged_adamw_8bit\",\n",
    "    logging_steps=5,\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=100,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    eval_steps=100,\n",
    "    learning_rate=5e-5,  # Reduced learning rate for stability\n",
    "    warmup_steps=100,\n",
    "    logging_dir=\"./logs\",\n",
    "    fp16=False,  # Disable fp16 for MPS stability\n",
    "    bf16=False,\n",
    "    max_grad_norm=0.3,\n",
    "    max_steps=500 if device.type == \"mps\" else -1,  # Slightly more steps\n",
    "    warmup_ratio=0.03,\n",
    "    group_by_length=False,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    report_to=\"none\",  # Disable wandb/tensorboard for simplicity\n",
    "    seed=42,\n",
    "    # Additional settings for MPS\n",
    "    dataloader_pin_memory=False if device.type == \"mps\" else True,\n",
    "    remove_unused_columns=False,  # Keep all columns to avoid issues\n",
    ")\n",
    "\n",
    "print(f\"Effective batch size: {training_args.per_device_train_batch_size * training_args.gradient_accumulation_steps}\")\n",
    "print(f\"Total training steps: {training_args.max_steps if training_args.max_steps > 0 else 'auto'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Initialize Trainer\n",
    "\n",
    "Set up the SFTTrainer for supervised fine-tuning:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Verifying training batches...\n",
      "Batch 0: label tokens = [349, 221]\n",
      "Batch 1: label tokens = [167, 170]\n",
      "Batch 2: label tokens = [440, 240]\n"
     ]
    }
   ],
   "source": [
    "# For causal LM you want no MLM masking, just pad\n",
    "pad_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer, mlm=False, pad_to_multiple_of=8\n",
    ")\n",
    "\n",
    "# Custom data collator for MPS compatibility\n",
    "def custom_data_collator(features):\n",
    "    batch = pad_collator(features)\n",
    "    # Ensure proper dtypes for MPS\n",
    "    if device.type == \"mps\":\n",
    "        # Convert to long for indices\n",
    "        batch[\"input_ids\"] = batch[\"input_ids\"].long()\n",
    "        batch[\"attention_mask\"] = batch[\"attention_mask\"].long()\n",
    "        if \"labels\" in batch:\n",
    "            batch[\"labels\"] = batch[\"labels\"].long()\n",
    "    return batch\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    data_collator=custom_data_collator,\n",
    "    tokenizer=tokenizer,\n",
    ")\n",
    "\n",
    "# Verify batches before training\n",
    "print(\"Verifying training batches...\")\n",
    "for step, batch in enumerate(trainer.get_train_dataloader()):\n",
    "    kept = (batch[\"labels\"] != -100).sum(-1).tolist()\n",
    "    print(f\"Batch {step}: label tokens = {kept}\")\n",
    "    if 0 in kept:\n",
    "        print(f\"Warning: Batch {step} has fully masked examples!\")\n",
    "    if step == 2:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Training Execution\n",
    "\n",
    "Start the fine-tuning process with error handling:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d372a462193241ffaa3af840c109bcee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.744, 'learning_rate': 2.5e-06, 'epoch': 0.0}\n",
      "{'loss': 2.9475, 'learning_rate': 5e-06, 'epoch': 0.0}\n"
     ]
    }
   ],
   "source": [
    "# Training with error handling\n",
    "try:\n",
    "    print(\"Starting training...\")\n",
    "    \n",
    "    # Clear cache if using MPS\n",
    "    if device.type == \"mps\":\n",
    "        torch.mps.empty_cache()\n",
    "    \n",
    "    # Ensure model is in training mode\n",
    "    model.train()\n",
    "    \n",
    "    # Train the model\n",
    "    trainer.train()\n",
    "    \n",
    "    print(\"\\n✅ Training completed successfully!\")\n",
    "    \n",
    "except RuntimeError as e:\n",
    "    error_msg = str(e).lower()\n",
    "    if \"out of memory\" in error_msg or \"mps\" in error_msg:\n",
    "        print(\"\\n❌ Memory error on MPS. Try these solutions:\")\n",
    "        print(\"1. Reduce per_device_train_batch_size to 1\")\n",
    "        print(\"2. Reduce gradient_accumulation_steps to 1\")\n",
    "        print(\"3. Reduce max sequence length\")\n",
    "        print(\"4. Use fewer training examples\")\n",
    "        print(\"5. Restart kernel and try again\")\n",
    "    elif \"nan\" in error_msg:\n",
    "        print(\"\\n❌ NaN loss detected. Try these solutions:\")\n",
    "        print(\"1. Reduce learning rate to 1e-5 or lower\")\n",
    "        print(\"2. Check if data is properly formatted\")\n",
    "        print(\"3. Use float32 instead of float16\")\n",
    "    else:\n",
    "        print(f\"\\n❌ Training error: {e}\")\n",
    "    raise e\n",
    "except Exception as e:\n",
    "    print(f\"\\n❌ Unexpected error: {e}\")\n",
    "    raise e"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Save the Fine-tuned Model\n",
    "\n",
    "Save the LoRA adapters and merged model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving LoRA adapters...\n",
      "✅ LoRA adapters saved to ./tinyllama-medical-qa-lora-adapters\n",
      "ℹ️ Skipping model merging on MPS (use adapters for inference)\n"
     ]
    }
   ],
   "source": [
    "# Save the model with proper handling for MPS\n",
    "try:\n",
    "    from peft import PeftModel\n",
    "    \n",
    "    lora_output_dir = \"./tinyllama-medical-qa-lora-adapters\"\n",
    "    \n",
    "    # Save PEFT model\n",
    "    print(\"Saving LoRA adapters...\")\n",
    "    model.save_pretrained(lora_output_dir)\n",
    "    tokenizer.save_pretrained(lora_output_dir)\n",
    "    print(f\"✅ LoRA adapters saved to {lora_output_dir}\")\n",
    "    \n",
    "    # Optional: merge adapters into the base weights\n",
    "    # Note: On MPS, merging might fail due to dtype issues\n",
    "    if device.type != \"mps\":\n",
    "        try:\n",
    "            print(\"Merging adapters...\")\n",
    "            merged_model = model.merge_and_unload()\n",
    "            merged_output_dir = \"./tinyllama-medical-qa-merged\"\n",
    "            merged_model.save_pretrained(merged_output_dir)\n",
    "            tokenizer.save_pretrained(merged_output_dir)\n",
    "            print(f\"✅ Merged model saved to {merged_output_dir}\")\n",
    "        except Exception as e:\n",
    "            print(f\"⚠️ Could not merge model: {e}\")\n",
    "            print(\"You can still use the LoRA adapters for inference\")\n",
    "    else:\n",
    "        print(\"ℹ️ Skipping model merging on MPS (use adapters for inference)\")\n",
    "    \n",
    "    # Clear MPS cache if on Apple Silicon\n",
    "    if device.type == \"mps\":\n",
    "        torch.mps.empty_cache()\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"❌ Error saving model: {e}\")\n",
    "    print(\"Try saving just the tokenizer separately:\")\n",
    "    print(\"tokenizer.save_pretrained('./tinyllama-tokenizer')\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Load Model for Inference\n",
    "\n",
    "Load the fine-tuned model for inference:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading base model...\n",
      "Loading LoRA adapters...\n",
      "✅ Model loaded for inference\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from peft import PeftModel\n",
    "\n",
    "# Reload tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(lora_output_dir, use_fast=True)\n",
    "\n",
    "# Pick dtype for MPS vs CUDA/CPU - use float32 for MPS\n",
    "dtype = torch.float32 if device.type == \"mps\" else torch.float16\n",
    "\n",
    "# Load base model\n",
    "print(\"Loading base model...\")\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    torch_dtype=dtype,\n",
    "    device_map=\"auto\" if device.type == \"cuda\" else None,\n",
    "    trust_remote_code=True,\n",
    ")\n",
    "\n",
    "# Attach LoRA adapters\n",
    "print(\"Loading LoRA adapters...\")\n",
    "model = PeftModel.from_pretrained(\n",
    "    base_model,\n",
    "    lora_output_dir,\n",
    "    torch_dtype=dtype,\n",
    ")\n",
    "\n",
    "# Move to device and set to eval mode\n",
    "if device.type == \"mps\":\n",
    "    model = model.to(device)\n",
    "model.eval()\n",
    "\n",
    "# Disable gradient computation for inference\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "print(\"✅ Model loaded for inference\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Inference Examples\n",
    "\n",
    "Test the fine-tuned model with medical questions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q: What are the side effects of ibuprofen?\n",
      "A: You are a helpful medical assistant. Answer accurately and concisely.\n",
      "--------------------------------------------------\n",
      "Q: How do you treat type 2 diabetes?\n",
      "A: Write a step-by-step guide on how to make homemade granola bars.\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "def medical_qa_inference(question: str,\n",
    "                         max_new_tokens: int = 128,\n",
    "                         temperature: float = 0.7,\n",
    "                         top_p: float = 0.9):\n",
    "    system_prompt = \"You are a helpful medical assistant. Answer accurately and concisely.\"\n",
    "    prompt = (\n",
    "        f\"<|system|>\\n{system_prompt}</s>\"\n",
    "        f\"<|user|>\\n{question}</s>\"\n",
    "        f\"<|assistant|>\\n\"\n",
    "    )\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "    with torch.no_grad():\n",
    "        out = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            temperature=temperature,\n",
    "            top_p=top_p,\n",
    "            do_sample=True,\n",
    "            pad_token_id=tokenizer.eos_token_id,\n",
    "            eos_token_id=tokenizer.eos_token_id,\n",
    "        )\n",
    "    # strip off the prompt tokens\n",
    "    generated = out[0, inputs[\"input_ids\"].size(-1):]\n",
    "    return tokenizer.decode(generated, skip_special_tokens=True)\n",
    "\n",
    "# example tests\n",
    "for q in [\n",
    "    \"What are the side effects of ibuprofen?\",\n",
    "    \"How do you treat type 2 diabetes?\"\n",
    "]:\n",
    "    ans = medical_qa_inference(q)\n",
    "    print(f\"Q: {q}\\nA: {ans}\\n{'-'*50}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. Performance Metrics and Memory Usage\n",
    "\n",
    "Display performance metrics for Apple Silicon:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📊 Apple Silicon (MPS) Performance Metrics\n",
      "==================================================\n",
      "Allocated: 8.43 GB\n",
      "Driver allocated: 9.33 GB\n",
      "\n",
      "Total params      : 1,112,664,064\n",
      "Trainable params  : 0\n",
      "Trainable percent : 0.00%\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "if device.type == \"mps\":\n",
    "    print(\"📊 Apple Silicon (MPS) Performance Metrics\")\n",
    "    print(\"=\"*50)\n",
    "    print(f\"Allocated: {torch.mps.current_allocated_memory()/1024**3:.2f} GB\")\n",
    "    print(f\"Driver allocated: {torch.mps.driver_allocated_memory()/1024**3:.2f} GB\")\n",
    "else:\n",
    "    print(\"📊 CPU Performance Metrics\")\n",
    "    print(\"=\"*50)\n",
    "    try:\n",
    "        import psutil\n",
    "        rss = psutil.Process().memory_info().rss\n",
    "        print(f\"RSS memory: {rss/1024**3:.2f} GB\")\n",
    "    except ImportError:\n",
    "        print(\"Install psutil for CPU memory stats\")\n",
    "\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f\"\\nTotal params      : {total_params:,}\")\n",
    "print(f\"Trainable params  : {trainable_params:,}\")\n",
    "print(f\"Trainable percent : {100*trainable_params/total_params:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 14. Cleanup and Best Practices\n",
    "\n",
    "Tips for optimizing performance on Apple Silicon:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ MPS cache cleared\n",
      "\n",
      "📝 Best Practices for Apple Silicon Fine-tuning:\n",
      "==================================================\n",
      "1. Use smaller batch sizes (1-2) to avoid OOM errors\n",
      "2. Enable gradient accumulation for larger effective batch sizes\n",
      "3. Use gradient checkpointing to save memory\n",
      "4. Monitor MPS memory usage regularly\n",
      "5. Clear MPS cache between training runs\n",
      "6. Use float32 precision for stability (not float16)\n",
      "7. Reduce sequence length if encountering memory issues\n",
      "8. Consider using smaller model variants for limited memory\n",
      "9. Set PYTORCH_MPS_HIGH_WATERMARK_RATIO=0.0 environment variable\n",
      "10. Use lower learning rates (5e-5 or lower) for stability\n"
     ]
    }
   ],
   "source": [
    "# Cleanup\n",
    "if device.type == \"mps\":\n",
    "    torch.mps.empty_cache()\n",
    "    print(\"✅ MPS cache cleared\")\n",
    "\n",
    "print(\"\\n📝 Best Practices for Apple Silicon Fine-tuning:\")\n",
    "print(\"=\"*50)\n",
    "print(\"1. Use smaller batch sizes (1-2) to avoid OOM errors\")\n",
    "print(\"2. Enable gradient accumulation for larger effective batch sizes\")\n",
    "print(\"3. Use gradient checkpointing to save memory\")\n",
    "print(\"4. Monitor MPS memory usage regularly\")\n",
    "print(\"5. Clear MPS cache between training runs\")\n",
    "print(\"6. Use float32 precision for stability (not float16)\")\n",
    "print(\"7. Reduce sequence length if encountering memory issues\")\n",
    "print(\"8. Consider using smaller model variants for limited memory\")\n",
    "print(\"9. Set PYTORCH_MPS_HIGH_WATERMARK_RATIO=0.0 environment variable\")\n",
    "print(\"10. Use lower learning rates (5e-5 or lower) for stability\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "This notebook demonstrated how to fine-tune TinyLlama on medical QA data using LoRA on Apple Silicon Macs. The implementation includes:\n",
    "\n",
    "- ✅ Automatic device detection (MPS/CPU)\n",
    "- ✅ Memory-efficient training with LoRA\n",
    "- ✅ Apple Silicon-specific optimizations\n",
    "- ✅ Comprehensive error handling\n",
    "- ✅ Medical QA dataset preprocessing\n",
    "- ✅ Inference examples with real medical questions\n",
    "\n",
    "The fine-tuned model can now answer medical questions with improved accuracy while maintaining efficiency on Apple Silicon hardware."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tinyllama_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
